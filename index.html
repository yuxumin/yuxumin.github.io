<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xumin Yu</title>
  
  <meta name="author" content="Xumin Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xumin Yu</name>
              </p>
              <p> 
                I am a second year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. Jiwen Lu. In 2020, I obtained my B.Eng. in the Department of Electronic Engineering, Tsinghua University.
              </p>
              <p>
              I am broadly interested in computer vision and deep learning. My current research focuses on 3D vision and Video analysis.
              </p>
              <p style="text-align:center">
                <a href="mailto:yuxm20@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=zfDZMZAAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/yuxumin"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/yxm.png">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2021-07:</b> 2 papers (including 1 oral) on 3D vision and video understanding are accepted to <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PoinTr.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</papertitle>
              <br>
              <strong>Xumin Yu*</strong>, <a href="https://raoyongming.github.io/"> Yongming Rao</a> *,  Ziyi Wang, Zuyan Liu, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>

              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
			  <a href="https://arxiv.org/abs/2108.08839">[arXiv]</a> <a href="files/PoinTr_supp.pdf">[supp]</a> <a href="https://github.com/yuxumin/PoinTr/">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/401928647">[中文解读 (by CVer)]</a>
              <br>
              <p>PoinTr is a transformer-based model for point cloud completion. By representing the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ a transformer encoder-decoder architecture for generation. We also propose two more challenging benchmarks ShapeNet-55/34 with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CoRe.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Group-aware Contrastive Regression for Action Quality Assessment</papertitle>
              <br>
              <strong>Xumin Yu*</strong>, <a href="https://raoyongming.github.io/"> Yongming Rao*</a>,  <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
                <a href="https://arxiv.org/abs/2108.07797">[arXiv]</a> <a href="https://github.com/yuxumin/CoRe">[Code]</a>
              <br>
              <p> We propose a new contrastive regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for assessment. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GIN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Graph Interaction Networks for Relation Transfer in Human Activity Videos</papertitle>
              <br>
			        <a href="https://andytang15.github.io/"> Yansong Tang</a>, <a href="https://weiyithu.github.io/"> Yi Wei </a>, <strong>Xumin Yu</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2020
	      <br>
              <a href="files/GIN_paper.pdf">[Paper]</a> 		    
              <br>
              <p> We propose a graph interaction networks (GINs) model for transferring relation knowledge across two graphs two different scenarios for video
analysis, including a new proposed setting for unsupervised skeleton-based action recognition across different datasets, and supervised group activity recognition with multi-modal inputs.
		        </td>
          </tr>	

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/finelabel.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning fine-grained estimation of physiological states from coarse-grained labels by distribution restoration</papertitle>
              <br>
              <a href="https://www.qinzy.tech/"> Zengyi Qin </a>, <a href="http://jschenthu.weebly.com/"> Jiansheng Chen </a>, <a href="https://zhenyujiang.me/"> Zhenyu Jiang </a>, <strong>Xumin Yu</strong>, Chunhua Hu, Yu Ma, Suhua Miao and Rongsong Zhou
              <br>
              <em> <strong> Scientific Reports</strong> </em>, 2020
              <br>
              <a href="https://www.nature.com/articles/s41598-020-79007-5">[Paper]</a> <a href="https://github.com/Zengyi-Qin/fine-biostate">[Code]</a>
              <br>
              <p></p>
              <p>Our method allows machine learning algorithms to perform fine-grained estimation of physiological states (e.g., sleep depth) even if the training labels are coarse-grained.</p>
            </td>
          </tr>

          </tbody></table>
<!-- 
          <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full"> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
  

          <!-- <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/finelabel.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning fine-grained estimation of physiological states from coarse-grained labels by distribution restoration</papertitle>
              <br>
              <a href="https://www.qinzy.tech/"> Zengyi Qin </a>, <a href="http://jschenthu.weebly.com/"> Jiansheng Chen </a>, <a href="https://zhenyujiang.me/"> Zhenyu Jiang </a>, <strong>Xumin Yu</strong>, Chunhua Hu, Yu Ma, Suhua Miao and Rongsong Zhou
              <br>
              <em> <strong> Scientific Reports</strong> </em>, 2020
              <br>
              <a href="https://www.nature.com/articles/s41598-020-79007-5">[Paper]</a> <a href="https://github.com/Zengyi-Qin/fine-biostate">[Code]</a>
              <br>
              <p></p>
              <p>Our method allows machine learning algorithms to perform fine-grained estimation of physiological states (e.g., sleep depth) even if the training labels are coarse-grained.</p>
            </td>
          </tr> -->

          <!-- </tbody></table> -->

      </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> Excellent Undergraduate in Tsinghua University, 2020 </li>
                <li style="margin: 5px;"> The First Prize of Microsoft Imagine Cup, China Finals, 2018 </li>
              </p>
            </td>
          </tr>
        </tbody></table>
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> Tutorial on Deep Reinforcement Learning for Computer Vision at CVPR 2019 <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2018-2021, ICML 2019-2021, ICCV 2019-2021, NeurIPS 2019-2021, ICLR 2021-2022, ECCV 2020, AAAI 2020-2021, WACV 2020-2022, ACCV 2018-2020, ICME 2019-2021, PRCV 2021, ICPR 2018-2020, ICIP 2018-2019
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b>  IJCV, T-IP, T-MM, T-Cybernetics, Pattern Recognition
              </li>
            </p>
          </td>
        </tr>
      </tbody></table> -->
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=Rz_KEqj91-8tEwaxNr0iid2Aku0rJ7hLFB8LoMCwzQI"></script>
    </div>        
	  <br>
	    &copy; Yu Xumin | Last updated: August 3, 2021
</center></p>
</body>

</html>
